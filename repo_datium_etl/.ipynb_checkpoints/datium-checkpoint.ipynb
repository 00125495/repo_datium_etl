{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import env as env\n",
    "import os\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cape_privacy as cape\n",
    "from cape_privacy.pandas import dtypes\n",
    "from cape_privacy.pandas import transformations as tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datiumETL:\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        \"\"\"\n",
    "        Object initialization for data management. It takes one argument, which is the file name.\n",
    "        \"\"\"\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        load csv file\n",
    "        \"\"\"\n",
    "        extension = os.path.splitext(self.file_name)[1][1:]\n",
    "        \n",
    "        if extension == \"csv\":\n",
    "            df=pd.read_csv(self.file_name)\n",
    "            return df\n",
    "        elif extension == \"json\":\n",
    "            df=pd.read_json(self.file_name, lines=True)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"not valid extension...\")\n",
    "    \n",
    "    def get_summary(self, df):\n",
    "        \"\"\"\n",
    "        Option 1 : manual data exploration\n",
    "        \"\"\"\n",
    "        types = df.dtypes\n",
    "        counts = df.apply(lambda x: x.count())\n",
    "        uniques = df.apply(lambda x: [x.unique()])\n",
    "        nas = df.apply(lambda x: x.isnull().sum())\n",
    "        distincts = df.apply(lambda x: x.unique().shape[0])\n",
    "        missing = (df.isnull().sum() / df.shape[0]) * 100\n",
    "        sk = df.skew()\n",
    "        krt = df.kurt()\n",
    "        \n",
    "        print('Data shape:', df.shape)\n",
    "\n",
    "        cols = ['Type', 'Total count', 'Null Values', 'Distinct Values', 'Missing Ratio', 'Unique Values', 'Skewness', 'Kurtosis']\n",
    "        dtls = pd.concat([types, counts, nas, distincts, missing, uniques, sk, krt], axis=1, sort=False)\n",
    "    \n",
    "        dtls.columns = cols\n",
    "        return dtls\n",
    "        \n",
    "    def get_missing_pct(self, df):\n",
    "\n",
    "        \"\"\"\n",
    "        This function is to calculate the total missing value percentage\n",
    "\n",
    "        \"\"\"\n",
    "        # get the number of missing data points per column\n",
    "        missing_values_count = df.isnull().sum()\n",
    "\n",
    "        # how many total missing values do we have?\n",
    "        total_cells = np.product(df.shape)\n",
    "        total_missing = missing_values_count.sum()\n",
    "\n",
    "        # percent of data that is missing\n",
    "        percent_missing = (total_missing/total_cells) * 100\n",
    "        \n",
    "        return \"Total missing data percentage is %.4f\" % (percent_missing)\n",
    "    \n",
    "    def get_date_fix(self, date_value):\n",
    "\n",
    "        DATE_FORMATS = ['%A, %B %dth, %Y', '%B %dth, %Y', '%Y-%m-%dTEST', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "        for date_format in DATE_FORMATS:\n",
    "            try:\n",
    "                my_date = datetime.datetime.strptime(date_value, date_format)\n",
    "                return my_date\n",
    "            except ValueError:\n",
    "                pass\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            my_date = '1900-01-01 00:00:00'\n",
    "            return my_date\n",
    "        \n",
    "    def get_convert_sec_date(self, df, col_name):\n",
    "        \n",
    "        df[col_name]=pd.to_datetime(df[col_name], unit='s', errors='coerce')\n",
    "        \n",
    "        #return df\n",
    "\n",
    "    def get_replace(self, df, column_name, original_values, new_values ):\n",
    "        \"\"\" \n",
    "        To replace non numeric values with new values and convert to float\n",
    "        column_name: column name for replace operation \n",
    "        original_values : list of original values\n",
    "        new_values: list of new values\n",
    "        \"\"\"\n",
    "        if df[column_name].isin(original_values).any():\n",
    "            df[column_name].replace(original_values,new_values, inplace=True)\n",
    "            df[column_name].fillna(np.nan)\n",
    "            df[column_name] = df[column_name].astype('bool')\n",
    "        return df[column_name]  \n",
    "    \n",
    "    def get_stand_decimal(self, df, column_name, decimal_points):\n",
    "        \n",
    "        df[column_name] = df[column_name].replace(['None', 'pyint'], [np.nan,np.nan]).fillna(0.00).astype('float64').round(decimals=2)\n",
    "        \n",
    "    def write_csv(self, df, obf_name):\n",
    "        compression_opts = dict(method='zip',archive_name=obf_name+'.csv')\n",
    "        df.to_csv(env.processed_data_dir+obf_name+'.zip', index=False,compression=compression_opts, sep='|')\n",
    "        print(\"Export Completed for \"+ obf_name)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = datiumETL(env.raw_data_file)\n",
    "data_json = datiumETL(env.raw_data_file_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv=data_csv.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json=data_json.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[\"created_at\"]=df_csv[\"created_at\"].apply(data_csv.get_date_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[\"is_claimed\"]= data_csv.get_replace(df_csv,\"is_claimed\",['fal_se', 'truee']\n",
    "                                                 , [False,True] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv.get_convert_sec_date(df_csv, \"last_login\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv.get_stand_decimal(df_csv, \"paid_amount\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy based encryption \n",
    "\n",
    "policy = cape.parse_policy(env.policy_file)\n",
    "caped_df = cape.apply_policy(policy, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv.write_csv(caped_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details_temp=pd.DataFrame(df_json.user_details.values.tolist())\n",
    "\n",
    "user_details=pd.DataFrame.from_records(user_details_temp)[['name','dob','address','username','password','national_id']]\n",
    "\n",
    "user_details.name=\"user_details\"\n",
    "\n",
    "data_json.write_csv(user_details, \"user_details\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telephone_numbers=pd.DataFrame.from_records(user_details_temp)[['name', 'telephone_numbers']].fillna('No Status')\n",
    "telephone_numbers[\"telephone_numbers\"]=telephone_numbers[\"telephone_numbers\"].apply(lambda x:','.join(map(str, x)))\n",
    "\n",
    "telephone_numbers.name=\"telephone_numbers\"\n",
    "\n",
    "data_json.write_csv(telephone_numbers, \"telephone_numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_history_temp=pd.DataFrame(df_json.jobs_history.values.tolist())[0]\n",
    "\n",
    "jobs_history=pd.DataFrame.from_records(jobs_history_temp)\n",
    "\n",
    "jobs_history.name=\"jobs_history\"\n",
    "\n",
    "data_json.write_csv(jobs_history, \"jobs_history\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
